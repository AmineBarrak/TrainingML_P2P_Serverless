# -*- coding: utf-8 -*-
"""ModelManager.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HpkkXKOiNmvm-ZbQ8-FOxR6STSkR9QIp
"""

import torch 
import torch.nn as nn 
import torch.nn.functional as F  
import torch.optim as optim
import torchvision 
import torchvision.datasets as datasets 
from torch.distributed.rpc import RRef, remote, rpc_async 
from time import sleep, time
import sys
import numpy as np


class MobileNet_Block(nn.Module):
    '''Depthwise conv + Pointwise conv'''
    def __init__(self, in_planes, out_planes, stride=1):
        super(MobileNet_Block, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)
        self.bn2 = nn.BatchNorm2d(out_planes)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.relu(self.bn2(self.conv2(out)))
        return out


class MobileNet(nn.Module):
    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1
    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]

    def __init__(self, num_classes=10):
        super(MobileNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.layers = self._make_layers(in_planes=32)
        self.linear = nn.Linear(1024, num_classes)

    def _make_layers(self, in_planes):
        layers = []
        for x in self.cfg:
            out_planes = x if isinstance(x, int) else x[0]
            stride = 1 if isinstance(x, int) else x[1]
            layers.append(MobileNet_Block(in_planes, out_planes, stride))
            in_planes = out_planes
        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layers(out)
        out = F.avg_pool2d(out, 2)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out
        
def select_loss(loss_fn):

  losses = {"NLL":nn.NLLLoss,"cross_entropy":nn.CrossEntropyLoss} 
  if loss_fn in losses.keys(): 
    return losses[loss_fn]() 
  else: 
    print("The selected loss function is undefined, available losses are: ", losses.keys())

def select_model(dataset,model):
  models={ 'resnet18':torchvision.models.resnet18,
                'resnet34':torchvision.models.resnet34,
                'resnet50':torchvision.models.resnet50,
                'resnet152':torchvision.models.resnet152,
	            	'inception':torchvision.models.inception_v3,
	             	'vgg16':torchvision.models.vgg16,
	             	'vgg19':torchvision.models.vgg19, 
		            'vgg11':torchvision.models.vgg11,
	            	'squeeznet11':torchvision.models.squeezenet1_1,
	             	'mobilenet-v2':torchvision.models.mobilenet_v2,
	             	'mnasnet0_5':torchvision.models.mnasnet0_5,
		             'densenet121': torchvision.models.densenet121, 
		             'mobilenet-v3-small': torchvision.models.mobilenet_v3_small,
		             'mobilenet' : MobileNet
		  
		            
      
  } 
  datasets= { "cifar10":10,"mnist":10} 
  if dataset in datasets.keys():
    num_classes = datasets[dataset] 
  else:
    print("The specified dataset is undefined, available datasets are: ", datasets.keys())


  if model in models.keys():
    model = models[model](num_classes=num_classes) 
  else:
    print("The specified model is undefined, available models are: ", models.keys())


  return model

def select_optimizer(model,optimizer,lr):
  optimizers={'sgd': optim.SGD,
		'adam': optim.Adam,
		'adamw':optim.AdamW,
		'rmsprop': optim.RMSprop,
		'adagrad': optim.Adagrad} 
  if optimizer in optimizers.keys():
    return optimizers[optimizer](model.parameters(),lr=lr) 
    
    
    
    
    
class ModelPytorch:
    def __init__(self, dataset, model_str):
        self.model = select_model(dataset,model_str)
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)

    def step(self, epoch, step, minibatch):


        features = minibatch['images']

        labels = minibatch['labels']
        self.optimizer.zero_grad()
        outputs = self.model(features)
        loss = self.criterion(outputs, labels)


        loss.backward() 
        grad = [torch.reshape(p.grad,(-1,)) for p in self.model.parameters()]
        gradients= torch.cat(grad)
        return loss, gradients, outputs, labels

    def get_significant_updates(self, updates):
        # For simplicity, return all model parameters as updates
        return updates
        
    def apply_update(self, average):
        cur_pos = 0
        average_tensor = torch.cat([torch.flatten(grad) for grad in average])
        for param in self.model.parameters():
            param_size = param.numel()
            param.grad = average_tensor[cur_pos:cur_pos + param_size].view(param.size()).detach()
            cur_pos += param_size
        self.optimizer.step()
    
    def get_weights(self):
        # Return the current weights of the model
        # You may need to adjust this based on how your model stores weights
        return [param.data for param in self.parameters()]

    def aggregate_model(self, received_weights):
        # Aggregate the received weights into the current model
        # You may need to adjust this based on how your model updates weights
        for param, received_param in zip(self.parameters(), received_weights):
            param.data += received_param

    def parameters(self):
        return [param for param in self.model.parameters() ] 



def get_model(dataset, model_str, **kwargs):
    return ModelPytorch(dataset, model_str)
